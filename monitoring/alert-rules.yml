# Prometheus Alert Rules for DeFiLlama Self-hosted Supabase
# Priority levels: P1 (Critical), P2 (High), P3 (Medium)

groups:
  # System-level alerts (P1)
  - name: system_critical
    interval: 30s
    rules:
      - alert: HighCPUUsage
        expr: 100 - (avg by (instance) (rate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 80
        for: 5m
        labels:
          severity: P1
          component: system
        annotations:
          summary: "High CPU usage on {{ $labels.instance }}"
          description: "CPU usage is above 80% (current: {{ $value }}%)"
          
      - alert: HighMemoryUsage
        expr: (1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100 > 85
        for: 5m
        labels:
          severity: P1
          component: system
        annotations:
          summary: "High memory usage on {{ $labels.instance }}"
          description: "Memory usage is above 85% (current: {{ $value }}%)"
          
      - alert: DiskSpaceLow
        expr: (1 - (node_filesystem_avail_bytes{fstype!="tmpfs"} / node_filesystem_size_bytes{fstype!="tmpfs"})) * 100 > 85
        for: 5m
        labels:
          severity: P1
          component: system
        annotations:
          summary: "Low disk space on {{ $labels.instance }}"
          description: "Disk usage is above 85% (current: {{ $value }}%)"
          
      - alert: ServiceDown
        expr: up == 0
        for: 1m
        labels:
          severity: P1
          component: system
        annotations:
          summary: "Service {{ $labels.job }} is down"
          description: "{{ $labels.job }} on {{ $labels.instance }} has been down for more than 1 minute"

  # Database alerts (P1)
  - name: database_critical
    interval: 30s
    rules:
      - alert: PostgreSQLDown
        expr: pg_up == 0
        for: 1m
        labels:
          severity: P1
          component: database
        annotations:
          summary: "PostgreSQL is down"
          description: "PostgreSQL database has been down for more than 1 minute"
          
      - alert: PostgreSQLTooManyConnections
        expr: postgres:connections:total > 900
        for: 5m
        labels:
          severity: P1
          component: database
        annotations:
          summary: "PostgreSQL has too many connections"
          description: "PostgreSQL has {{ $value }} connections (max: 1000)"

      - alert: PostgreSQLHighReplicationLag
        expr: postgres:replication_lag:seconds > 30
        for: 5m
        labels:
          severity: P1
          component: database
        annotations:
          summary: "PostgreSQL replication lag is high"
          description: "Replication lag is {{ $value }} seconds (threshold: 30s)"

      - alert: PostgreSQLDeadlocks
        expr: postgres:deadlocks:rate5m > 0
        for: 1m
        labels:
          severity: P2
          component: database
        annotations:
          summary: "PostgreSQL deadlocks detected"
          description: "{{ $value }} deadlocks per second detected"

  # Redis alerts (P1)
  - name: redis_critical
    interval: 30s
    rules:
      - alert: RedisDown
        expr: redis_up == 0
        for: 1m
        labels:
          severity: P1
          component: cache
        annotations:
          summary: "Redis is down"
          description: "Redis has been down for more than 1 minute"
          
      - alert: RedisHighMemoryUsage
        expr: (redis_memory_used_bytes / redis_memory_max_bytes) * 100 > 90
        for: 5m
        labels:
          severity: P1
          component: cache
        annotations:
          summary: "Redis memory usage is high"
          description: "Redis memory usage is {{ $value }}% (threshold: 90%)"
          
      - alert: RedisHighConnectionCount
        expr: redis_connected_clients > 9000
        for: 5m
        labels:
          severity: P2
          component: cache
        annotations:
          summary: "Redis has too many connections"
          description: "Redis has {{ $value }} connections (max: 10000)"
          
      - alert: RedisHighEvictionRate
        expr: rate(redis_evicted_keys_total[5m]) > 100
        for: 5m
        labels:
          severity: P2
          component: cache
        annotations:
          summary: "Redis eviction rate is high"
          description: "Redis is evicting {{ $value }} keys per second"

  # WebSocket alerts (P1)
  - name: websocket_critical
    interval: 30s
    rules:
      - alert: WebSocketServerDown
        expr: up{job="defillama-websocket"} == 0
        for: 1m
        labels:
          severity: P1
          component: websocket
        annotations:
          summary: "WebSocket server is down"
          description: "WebSocket server has been down for more than 1 minute"
          
      - alert: WebSocketHighConnectionCount
        expr: websocket_connections_total > 45000
        for: 5m
        labels:
          severity: P2
          component: websocket
        annotations:
          summary: "WebSocket connection count is high"
          description: "{{ $value }} connections (max: 50000)"
          
      - alert: WebSocketHighLatency
        expr: histogram_quantile(0.95, rate(websocket_message_duration_seconds_bucket[5m])) > 0.1
        for: 5m
        labels:
          severity: P2
          component: websocket
        annotations:
          summary: "WebSocket latency is high"
          description: "P95 latency is {{ $value }}s (threshold: 0.1s)"
          
      - alert: WebSocketHighErrorRate
        expr: rate(websocket_errors_total[5m]) > 10
        for: 5m
        labels:
          severity: P2
          component: websocket
        annotations:
          summary: "WebSocket error rate is high"
          description: "{{ $value }} errors per second"

  # API Gateway alerts (P1)
  - name: api_gateway_critical
    interval: 30s
    rules:
      - alert: KongDown
        expr: up{job="supabase-kong"} == 0
        for: 1m
        labels:
          severity: P1
          component: api_gateway
        annotations:
          summary: "Kong API Gateway is down"
          description: "Kong has been down for more than 1 minute"
          
      - alert: KongHighErrorRate
        expr: rate(kong_http_status{code=~"5.."}[5m]) / rate(kong_http_status[5m]) > 0.05
        for: 5m
        labels:
          severity: P1
          component: api_gateway
        annotations:
          summary: "Kong error rate is high"
          description: "{{ $value | humanizePercentage }} of requests are failing"
          
      - alert: KongHighLatency
        expr: histogram_quantile(0.95, rate(kong_latency_bucket[5m])) > 1000
        for: 5m
        labels:
          severity: P2
          component: api_gateway
        annotations:
          summary: "Kong latency is high"
          description: "P95 latency is {{ $value }}ms (threshold: 1000ms)"
          
      - alert: KongHighRequestRate
        expr: rate(kong_http_status[5m]) > 10000
        for: 5m
        labels:
          severity: P3
          component: api_gateway
        annotations:
          summary: "Kong request rate is high"
          description: "{{ $value }} requests per second (threshold: 10000)"

  # Supabase Realtime alerts (P1)
  - name: realtime_critical
    interval: 30s
    rules:
      - alert: RealtimeDown
        expr: up{job="supabase-realtime"} == 0
        for: 1m
        labels:
          severity: P1
          component: realtime
        annotations:
          summary: "Supabase Realtime is down"
          description: "Realtime service has been down for more than 1 minute"
          
      - alert: RealtimeHighChannelCount
        expr: realtime_channels_total > 45000
        for: 5m
        labels:
          severity: P2
          component: realtime
        annotations:
          summary: "Realtime channel count is high"
          description: "{{ $value }} channels (max: 50000)"
          
      - alert: RealtimeHighEventRate
        expr: rate(realtime_events_total[5m]) > 90000
        for: 5m
        labels:
          severity: P2
          component: realtime
        annotations:
          summary: "Realtime event rate is high"
          description: "{{ $value }} events per second (max: 100000)"

  # Business metrics alerts (P2)
  - name: business_metrics
    interval: 1m
    rules:
      - alert: LowSubscriptionRate
        expr: rate(defillama_subscriptions_total[5m]) < 1
        for: 10m
        labels:
          severity: P3
          component: business
        annotations:
          summary: "Low subscription rate"
          description: "Only {{ $value }} subscriptions per second in the last 10 minutes"
          
      - alert: HighUnsubscribeRate
        expr: rate(defillama_unsubscriptions_total[5m]) > rate(defillama_subscriptions_total[5m])
        for: 10m
        labels:
          severity: P2
          component: business
        annotations:
          summary: "Unsubscribe rate exceeds subscribe rate"
          description: "More users are unsubscribing than subscribing"
          
      - alert: NoEventsProcessed
        expr: rate(defillama_events_processed_total[5m]) == 0
        for: 10m
        labels:
          severity: P2
          component: business
        annotations:
          summary: "No events processed"
          description: "No events have been processed in the last 10 minutes"

  # Docker container alerts (P2)
  - name: docker_alerts
    interval: 30s
    rules:
      - alert: ContainerHighCPU
        expr: rate(container_cpu_usage_seconds_total{name!=""}[5m]) * 100 > 80
        for: 5m
        labels:
          severity: P2
          component: docker
        annotations:
          summary: "Container {{ $labels.name }} has high CPU usage"
          description: "CPU usage is {{ $value }}% (threshold: 80%)"
          
      - alert: ContainerHighMemory
        expr: (container_memory_usage_bytes{name!=""} / container_spec_memory_limit_bytes{name!=""}) * 100 > 85
        for: 5m
        labels:
          severity: P2
          component: docker
        annotations:
          summary: "Container {{ $labels.name }} has high memory usage"
          description: "Memory usage is {{ $value }}% (threshold: 85%)"
          
      - alert: ContainerRestarting
        expr: rate(container_last_seen{name!=""}[5m]) > 0
        for: 5m
        labels:
          severity: P2
          component: docker
        annotations:
          summary: "Container {{ $labels.name }} is restarting"
          description: "Container has restarted {{ $value }} times in the last 5 minutes"

